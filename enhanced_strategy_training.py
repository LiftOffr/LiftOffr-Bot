#!/usr/bin/env python3
"""
Enhanced Strategy Training for ARIMA and Adaptive Integration

This script implements a sophisticated training pipeline that combines both
ARIMA and Adaptive strategies with advanced ML models to achieve 90% win rate
and 1000% returns. It implements:

1. Dual-strategy input features for ML models
2. Cross-strategy signal integration
3. Hyper-optimized training with asymmetric loss functions
4. Reinforced position sizing based on signal confidence
5. Integrated backtesting with realistic market simulation

The training process optimizes for maximum returns while maintaining high accuracy,
allowing for extreme leverage during high-confidence predictions.
"""

import os
import json
import time
import logging
import numpy as np
import pandas as pd
import tensorflow as tf
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Union, Any, Optional
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import train_test_split

# Import local modules
import arima_strategy
import fixed_strategy
from advanced_ensemble_model import DynamicWeightedEnsemble, create_hybrid_model
from advanced_ml_integration import MLModelIntegrator
from adaptive_hyperparameter_tuning import AdaptiveHyperparameterTuner
from ml_model_integrator import MLModelIntegrator
from model_collaboration_integrator import ModelCollaborationIntegrator
from enhanced_tcn_model import EnhancedTCNModel
from attention_gru_model import AttentionGRUModel
from explainable_ai_integration import ExplainableAI
from dynamic_position_sizing import DynamicPositionSizer
from enhanced_backtesting import EnhancedBacktester

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('enhanced_training.log')
    ]
)
logger = logging.getLogger(__name__)

# Constants
CONFIG_PATH = "ml_enhanced_config.json"
TRAINING_DATA_DIR = "training_data"
MODELS_DIR = "models"
RESULTS_DIR = "optimization_results"

class EnhancedStrategyTrainer:
    """
    Advanced ML trainer that integrates ARIMA and Adaptive strategies
    for maximum performance.
    """
    def __init__(
        self,
        trading_pairs: List[str],
        config_path: str = CONFIG_PATH,
        epochs: int = 300,
        batch_size: int = 32,
        lookback: int = 100,
        forecast_horizon: int = 10,
        validation_split: float = 0.2,
        test_split: float = 0.1,
        min_required_samples: int = 5000,
        max_leverage: int = 125,
        target_win_rate: float = 0.9,
        target_return_pct: float = 1000.0
    ):
        """
        Initialize the enhanced strategy trainer
        
        Args:
            trading_pairs: List of trading pairs to train on
            config_path: Path to ML configuration file
            epochs: Number of training epochs
            batch_size: Training batch size
            lookback: Number of historical candles to use for prediction
            forecast_horizon: Number of future candles to predict
            validation_split: Fraction of data to use for validation
            test_split: Fraction of data to use for testing
            min_required_samples: Minimum number of samples required for training
            max_leverage: Maximum leverage to use
            target_win_rate: Target win rate (0.0-1.0)
            target_return_pct: Target return percentage (e.g., 1000.0 for 1000%)
        """
        self.trading_pairs = trading_pairs
        self.config_path = config_path
        self.epochs = epochs
        self.batch_size = batch_size
        self.lookback = lookback
        self.forecast_horizon = forecast_horizon
        self.validation_split = validation_split
        self.test_split = test_split
        self.min_required_samples = min_required_samples
        self.max_leverage = max_leverage
        self.target_win_rate = target_win_rate
        self.target_return_pct = target_return_pct
        
        # Ensure directories exist
        os.makedirs(TRAINING_DATA_DIR, exist_ok=True)
        os.makedirs(MODELS_DIR, exist_ok=True)
        os.makedirs(RESULTS_DIR, exist_ok=True)
        
        # Load configuration
        self.config = self._load_config()
        
        # Initialize hyperparameter tuner
        self.hyperparameter_tuner = AdaptiveHyperparameterTuner(
            model_type="ensemble",
            asset=trading_pairs[0],
            strategy_name="integrated"
        )
        
        # Initialize model integrator
        self.model_integrator = MLModelIntegrator()
        
        # Initialize collaborative integrator
        self.collaborative_integrator = ModelCollaborationIntegrator(
            trading_pairs=trading_pairs,
            enable_adaptive_weights=True
        )
        
        # Initialize dynamic position sizer
        self.position_sizer = DynamicPositionSizer(
            max_leverage=max_leverage,
            base_risk_pct=5.0,
            max_risk_pct=20.0
        )
        
        # Initialize backtester
        self.backtester = EnhancedBacktester(
            trading_pairs=trading_pairs,
            use_realistic_slippage=True,
            use_realistic_fees=True,
            initial_capital=20000.0
        )
        
        # Initialize strategies for feature integration
        self._initialize_strategies()
        
        logger.info(f"Enhanced Strategy Trainer initialized for {len(trading_pairs)} pairs")
        logger.info(f"Target win rate: {target_win_rate*100:.1f}%, Target return: {target_return_pct:.1f}%")
    
    def _load_config(self) -> Dict[str, Any]:
        """Load ML configuration from file"""
        try:
            if os.path.exists(self.config_path):
                with open(self.config_path, "r") as f:
                    config = json.load(f)
                logger.info(f"Loaded configuration from {self.config_path}")
                
                # Backup the original config
                backup_path = f"{self.config_path}.{datetime.now().strftime('%Y%m%d_%H%M%S')}.bak"
                with open(backup_path, "w") as f:
                    json.dump(config, f, indent=2)
                logger.info(f"Backed up configuration to {backup_path}")
                
                return config
            else:
                logger.warning(f"Configuration file {self.config_path} not found")
                return {}
        except Exception as e:
            logger.error(f"Error loading configuration: {e}")
            return {}
    
    def _initialize_strategies(self):
        """Initialize ARIMA and Adaptive strategies for feature extraction"""
        try:
            # Import at runtime to avoid circular imports
            from arima_strategy import ARIMAStrategy
            from fixed_strategy import AdaptiveStrategy
            
            self.arima_strategy = ARIMAStrategy(
                trading_pair=self.trading_pairs[0],
                timeframe="1h",
                sandbox=True,
                backtest=True
            )
            
            self.adaptive_strategy = AdaptiveStrategy(
                trading_pair=self.trading_pairs[0],
                timeframe="1h",
                sandbox=True,
                backtest=True
            )
            
            logger.info("Initialized ARIMA and Adaptive strategies")
        except Exception as e:
            logger.error(f"Error initializing strategies: {e}")
    
    def prepare_training_data(self, pair: str, timeframe: str = "1h") -> pd.DataFrame:
        """
        Prepare training data for a specific trading pair
        
        Args:
            pair: Trading pair
            timeframe: Timeframe (e.g., "1h", "4h", "1d")
            
        Returns:
            DataFrame with prepared training data
        """
        # Construct data file path
        pair_filename = pair.replace("/", "")
        data_path = os.path.join(TRAINING_DATA_DIR, f"{pair_filename}_{timeframe}.csv")
        
        # Check if data file exists
        if not os.path.exists(data_path):
            logger.warning(f"Data file {data_path} not found")
            return None
        
        try:
            # Load data
            df = pd.read_csv(data_path)
            
            # Convert timestamp to datetime
            if "timestamp" in df.columns:
                df["timestamp"] = pd.to_datetime(df["timestamp"])
            
            # Check if we have enough data
            if len(df) < self.min_required_samples:
                logger.warning(f"Not enough data for {pair}: {len(df)} < {self.min_required_samples}")
                return None
            
            # Calculate ARIMA strategy features
            arima_features = self._generate_arima_features(df)
            
            # Calculate Adaptive strategy features
            adaptive_features = self._generate_adaptive_features(df)
            
            # Merge features with price data
            df = df.merge(arima_features, on="timestamp", how="left")
            df = df.merge(adaptive_features, on="timestamp", how="left")
            
            # Calculate additional features
            df = self._calculate_additional_features(df, timeframe)
            
            # Forward-fill missing values
            df.fillna(method="ffill", inplace=True)
            
            # Drop any remaining NaN values
            df.dropna(inplace=True)
            
            # Calculate target variables
            df = self._calculate_targets(df)
            
            logger.info(f"Prepared {len(df)} samples for {pair}")
            return df
            
        except Exception as e:
            logger.error(f"Error preparing data for {pair}: {e}")
            return None
    
    def _generate_arima_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Generate ARIMA strategy features
        
        Args:
            df: DataFrame with price data
            
        Returns:
            DataFrame with ARIMA features
        """
        try:
            # Initialize ARIMA strategy for feature generation
            arima_features = pd.DataFrame(index=df.index)
            arima_features["timestamp"] = df["timestamp"]
            
            # Generate ARIMA predictions
            if hasattr(self, "arima_strategy"):
                predictions = []
                forecasts = []
                strengths = []
                
                # Process data in chunks to avoid memory issues
                chunk_size = 1000
                for i in range(0, len(df), chunk_size):
                    chunk = df.iloc[i:i+chunk_size].copy()
                    
                    # Get ARIMA predictions for this chunk
                    for j in range(len(chunk)):
                        try:
                            candle_data = chunk.iloc[j:j+1].copy()
                            signal, forecast, strength = self.arima_strategy.generate_signal_for_candle(candle_data)
                            predictions.append(1 if signal == "buy" else (-1 if signal == "sell" else 0))
                            forecasts.append(forecast)
                            strengths.append(strength)
                        except Exception as e:
                            predictions.append(0)
                            forecasts.append(None)
                            strengths.append(0)
                
                # Add to features
                arima_features["arima_prediction"] = predictions[:len(df)]
                arima_features["arima_forecast"] = forecasts[:len(df)]
                arima_features["arima_strength"] = strengths[:len(df)]
            
            return arima_features
        
        except Exception as e:
            logger.error(f"Error generating ARIMA features: {e}")
            return pd.DataFrame({"timestamp": df["timestamp"]})
    
    def _generate_adaptive_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Generate Adaptive strategy features
        
        Args:
            df: DataFrame with price data
            
        Returns:
            DataFrame with Adaptive features
        """
        try:
            # Initialize Adaptive strategy for feature generation
            adaptive_features = pd.DataFrame(index=df.index)
            adaptive_features["timestamp"] = df["timestamp"]
            
            # Generate Adaptive predictions
            if hasattr(self, "adaptive_strategy"):
                predictions = []
                strengths = []
                volatilities = []
                
                # Process data in chunks to avoid memory issues
                chunk_size = 1000
                for i in range(0, len(df), chunk_size):
                    chunk = df.iloc[i:i+chunk_size].copy()
                    
                    # Get Adaptive predictions for this chunk
                    for j in range(len(chunk)):
                        try:
                            candle_data = chunk.iloc[j:j+1].copy()
                            signal, strength, volatility = self.adaptive_strategy.generate_signal_for_candle(candle_data)
                            predictions.append(1 if signal == "buy" else (-1 if signal == "sell" else 0))
                            strengths.append(strength)
                            volatilities.append(volatility)
                        except Exception as e:
                            predictions.append(0)
                            strengths.append(0)
                            volatilities.append(0)
                
                # Add to features
                adaptive_features["adaptive_prediction"] = predictions[:len(df)]
                adaptive_features["adaptive_strength"] = strengths[:len(df)]
                adaptive_features["adaptive_volatility"] = volatilities[:len(df)]
            
            return adaptive_features
        
        except Exception as e:
            logger.error(f"Error generating Adaptive features: {e}")
            return pd.DataFrame({"timestamp": df["timestamp"]})
    
    def _calculate_additional_features(self, df: pd.DataFrame, timeframe: str) -> pd.DataFrame:
        """
        Calculate additional features for training
        
        Args:
            df: DataFrame with price and strategy features
            timeframe: Timeframe
            
        Returns:
            DataFrame with additional features
        """
        # Technical indicators
        df["rsi"] = self._calculate_rsi(df["close"], window=14)
        df["ema9"] = self._calculate_ema(df["close"], window=9)
        df["ema21"] = self._calculate_ema(df["close"], window=21)
        df["ema50"] = self._calculate_ema(df["close"], window=50)
        df["ema100"] = self._calculate_ema(df["close"], window=100)
        df["atr"] = self._calculate_atr(df, window=14)
        df["pct_change"] = df["close"].pct_change()
        df["pct_change_8h"] = df["close"].pct_change(8)
        df["pct_change_24h"] = df["close"].pct_change(24)
        
        # Bollinger Bands
        df["bb_middle"], df["bb_upper"], df["bb_lower"] = self._calculate_bollinger_bands(df["close"], window=20)
        df["bb_width"] = (df["bb_upper"] - df["bb_lower"]) / df["bb_middle"]
        
        # Keltner Channels
        df["kc_middle"], df["kc_upper"], df["kc_lower"] = self._calculate_keltner_channels(df, window=20)
        df["kc_width"] = (df["kc_upper"] - df["kc_lower"]) / df["kc_middle"]
        
        # Volatility measures
        df["volatility_1h"] = df["pct_change"].rolling(window=24).std()
        df["volatility_4h"] = df["pct_change"].rolling(window=96).std()
        df["volatility_1d"] = df["pct_change"].rolling(window=24*7).std()
        
        # Volume profiles
        df["volume_sma"] = df["volume"].rolling(window=20).mean()
        df["volume_ratio"] = df["volume"] / df["volume_sma"]
        
        # Trend strength
        df["adx"] = self._calculate_adx(df, window=14)
        
        # Strategy interaction features
        df["strategy_agreement"] = df["arima_prediction"] * df["adaptive_prediction"]
        df["strategy_combined_strength"] = df["arima_strength"] * df["adaptive_strength"]
        
        # Market regime
        df["bull_market"] = (df["ema9"] > df["ema21"]) & (df["ema21"] > df["ema50"]).astype(int)
        df["bear_market"] = (df["ema9"] < df["ema21"]) & (df["ema21"] < df["ema50"]).astype(int)
        df["neutral_market"] = 1 - df["bull_market"] - df["bear_market"]
        df["market_regime"] = (df["bull_market"] * 1) + (df["bear_market"] * -1) + (df["neutral_market"] * 0)
        
        # Calculate returns on different timeframes
        for period in [1, 2, 3, 4, 6, 8, 12, 24, 48, 96]:
            df[f"future_return_{period}"] = df["close"].pct_change(period).shift(-period)
        
        return df
    
    def _calculate_rsi(self, data: pd.Series, window: int = 14) -> pd.Series:
        """Calculate RSI for a price series"""
        delta = data.diff()
        gain = delta.where(delta > 0, 0).rolling(window=window).mean()
        loss = -delta.where(delta < 0, 0).rolling(window=window).mean()
        
        relative_strength = gain / loss
        rsi = 100 - (100 / (1 + relative_strength))
        return rsi
    
    def _calculate_ema(self, data: pd.Series, window: int) -> pd.Series:
        """Calculate EMA for a price series"""
        return data.ewm(span=window, adjust=False).mean()
    
    def _calculate_atr(self, df: pd.DataFrame, window: int = 14) -> pd.Series:
        """Calculate ATR for price data"""
        high_low = df["high"] - df["low"]
        high_close = np.abs(df["high"] - df["close"].shift())
        low_close = np.abs(df["low"] - df["close"].shift())
        
        tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        atr = tr.rolling(window=window).mean()
        return atr
    
    def _calculate_bollinger_bands(self, data: pd.Series, window: int = 20, num_std: float = 2.0) -> Tuple[pd.Series, pd.Series, pd.Series]:
        """Calculate Bollinger Bands for a price series"""
        sma = data.rolling(window=window).mean()
        std = data.rolling(window=window).std()
        upper_band = sma + (std * num_std)
        lower_band = sma - (std * num_std)
        return sma, upper_band, lower_band
    
    def _calculate_keltner_channels(self, df: pd.DataFrame, window: int = 20, atr_multiplier: float = 2.0) -> Tuple[pd.Series, pd.Series, pd.Series]:
        """Calculate Keltner Channels for price data"""
        ema = df["close"].ewm(span=window, adjust=False).mean()
        atr = self._calculate_atr(df, window)
        upper_channel = ema + (atr * atr_multiplier)
        lower_channel = ema - (atr * atr_multiplier)
        return ema, upper_channel, lower_channel
    
    def _calculate_adx(self, df: pd.DataFrame, window: int = 14) -> pd.Series:
        """Calculate ADX for price data"""
        # Placeholder for ADX calculation
        return pd.Series(50, index=df.index)  # Simplified for demonstration
    
    def _calculate_targets(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Calculate target variables for ML models
        
        Args:
            df: DataFrame with price and feature data
            
        Returns:
            DataFrame with added target variables
        """
        # Calculate price direction targets
        for period in [1, 2, 3, 4, 8, 12, 24]:
            # Future price change
            future_change = df["close"].shift(-period) - df["close"]
            # Binary direction (1 for up, 0 for down)
            df[f"target_direction_{period}"] = (future_change > 0).astype(int)
            # Threshold-based direction (1 for significant up, -1 for significant down, 0 for flat)
            threshold = df["atr"] * 0.5  # Use half ATR as significance threshold
            df[f"target_direction_thresh_{period}"] = 0
            df.loc[future_change > threshold, f"target_direction_thresh_{period}"] = 1
            df.loc[future_change < -threshold, f"target_direction_thresh_{period}"] = -1
            
            # Percentage change target (for regression)
            df[f"target_pct_change_{period}"] = future_change / df["close"] * 100
            
            # Volatility-adjusted return (Sharpe ratio style)
            vol = df["volatility_1h"].rolling(window=period).mean().shift(-period)
            vol_safe = vol.copy()
            vol_safe.loc[vol_safe < 0.0001] = 0.0001  # Avoid division by zero
            df[f"target_vol_adj_{period}"] = (future_change / df["close"]) / vol_safe
        
        # Calculate trading signals based on combined strategy
        df["target_signal"] = 0
        # Buy signal when both strategies agree on buy or ARIMA buy with high confidence
        buy_condition = ((df["arima_prediction"] > 0) & (df["adaptive_prediction"] > 0)) | \
                        ((df["arima_prediction"] > 0) & (df["arima_strength"] > 0.8))
        # Sell signal when both strategies agree on sell or ARIMA sell with high confidence
        sell_condition = ((df["arima_prediction"] < 0) & (df["adaptive_prediction"] < 0)) | \
                        ((df["arima_prediction"] < 0) & (df["arima_strength"] > 0.8))
        
        df.loc[buy_condition, "target_signal"] = 1
        df.loc[sell_condition, "target_signal"] = -1
        
        # Calculate optimal leverage based on future returns and strategy confidence
        for period in [4, 8, 12, 24]:
            future_return = df[f"future_return_{period}"]
            # Calculate ideal leverage (high leverage for high-confidence, profitable trades)
            confidence = (df["arima_strength"] + df["adaptive_strength"]) / 2
            
            # For positive returns, leverage scales with confidence
            positive_leverage = np.minimum(self.max_leverage * confidence * (future_return > 0), self.max_leverage)
            
            # For negative returns, inverse leverage (but limit to smaller values)
            negative_leverage = np.minimum(-5 * confidence * (future_return < 0), 0)
            
            # Combine the two
            df[f"target_leverage_{period}"] = positive_leverage + negative_leverage
            
            # Leverage adjustments: when returns are near zero, leverage should be zero
            small_return_mask = np.abs(future_return) < 0.001
            df.loc[small_return_mask, f"target_leverage_{period}"] = 0
        
        return df
    
    def prepare_model_inputs(self, data: pd.DataFrame, lookback: int, forecast_horizon: int) -> Tuple:
        """
        Prepare ML model inputs from processed data
        
        Args:
            data: Processed DataFrame with features and targets
            lookback: Number of historical candles to use
            forecast_horizon: Forecast horizon
            
        Returns:
            Tuple of (X_train, y_train, X_val, y_val, X_test, y_test, feature_scaler)
        """
        # Select features
        feature_columns = [
            # Price data
            "open", "high", "low", "close", "volume",
            
            # Strategy features
            "arima_prediction", "arima_strength", "arima_forecast",
            "adaptive_prediction", "adaptive_strength", "adaptive_volatility",
            "strategy_agreement", "strategy_combined_strength",
            
            # Technical indicators
            "rsi", "ema9", "ema21", "ema50", "ema100", "atr",
            "bb_middle", "bb_upper", "bb_lower", "bb_width",
            "kc_middle", "kc_upper", "kc_lower", "kc_width",
            "volatility_1h", "volatility_4h", "volatility_1d",
            "volume_ratio", "adx",
            
            # Market regime
            "bull_market", "bear_market", "neutral_market", "market_regime"
        ]
        
        # Select targets for different prediction horizons
        target_columns = [
            f"target_direction_{forecast_horizon}",
            f"target_direction_thresh_{forecast_horizon}",
            f"target_pct_change_{forecast_horizon}",
            f"target_vol_adj_{forecast_horizon}",
            f"target_leverage_{forecast_horizon}",
            "target_signal"
        ]
        
        # Ensure all columns are available
        feature_columns = [col for col in feature_columns if col in data.columns]
        target_columns = [col for col in target_columns if col in data.columns]
        
        # Create feature matrix and target vector
        features = data[feature_columns].values
        targets = data[target_columns].values
        
        # Scale features
        feature_scaler = RobustScaler()
        features_scaled = feature_scaler.fit_transform(features)
        
        # Create sequences
        X, y = self._create_sequences(features_scaled, targets, lookback)
        
        # Split into train, validation, and test sets
        n_samples = len(X)
        train_size = int(n_samples * (1 - self.validation_split - self.test_split))
        val_size = int(n_samples * self.validation_split)
        
        X_train, y_train = X[:train_size], y[:train_size]
        X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]
        X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]
        
        logger.info(f"Prepared {len(X_train)} training, {len(X_val)} validation, and {len(X_test)} test samples")
        
        return X_train, y_train, X_val, y_val, X_test, y_test, feature_scaler
    
    def _create_sequences(self, features: np.ndarray, targets: np.ndarray, lookback: int) -> Tuple[np.ndarray, np.ndarray]:
        """
        Create sequences for RNN training
        
        Args:
            features: Feature matrix (n_samples, n_features)
            targets: Target matrix (n_samples, n_targets)
            lookback: Number of historical candles to use
            
        Returns:
            Tuple of (X, y) where X is (n_sequences, lookback, n_features) and y is (n_sequences, n_targets)
        """
        n_samples = len(features)
        n_features = features.shape[1]
        n_targets = targets.shape[1]
        
        X = np.zeros((n_samples - lookback, lookback, n_features))
        y = np.zeros((n_samples - lookback, n_targets))
        
        for i in range(lookback, n_samples):
            X[i-lookback] = features[i-lookback:i]
            y[i-lookback] = targets[i]
        
        return X, y
    
    def create_model(self, input_shape: Tuple[int, int], output_shape: int, hyperparameters: Dict[str, Any] = None) -> tf.keras.Model:
        """
        Create deep learning model with hyperparameters
        
        Args:
            input_shape: Input shape (lookback, n_features)
            output_shape: Number of output targets
            hyperparameters: Model hyperparameters
            
        Returns:
            Keras model
        """
        # Default hyperparameters
        default_hyperparams = {
            "rnn_units": 64,
            "rnn_layers": 2,
            "dropout_rate": 0.3,
            "learning_rate": 0.001,
            "use_attention": True,
            "use_tcn": True,
        }
        
        # Combine with provided hyperparameters
        hp = {**default_hyperparams, **(hyperparameters or {})}
        
        # Input layer
        inputs = tf.keras.layers.Input(shape=input_shape)
        
        # Create a hybrid model that combines TCN, LSTM, and Attention components
        # First branch: TCN network
        if hp["use_tcn"]:
            # TCN parameters
            nb_filters = 64
            kernel_size = 3
            nb_stacks = 2
            dilations = [1, 2, 4, 8, 16]
            
            from tcn import TCN
            tcn_layer = TCN(
                nb_filters=nb_filters,
                kernel_size=kernel_size,
                nb_stacks=nb_stacks,
                dilations=dilations,
                padding='causal',
                use_skip_connections=True,
                dropout_rate=hp["dropout_rate"],
                return_sequences=True
            )(inputs)
            
            tcn_pooled = tf.keras.layers.GlobalAveragePooling1D()(tcn_layer)
        else:
            tcn_pooled = None
        
        # Second branch: LSTM/GRU network with attention
        if hp["use_attention"]:
            # Bidirectional RNN layers
            rnn_layer = inputs
            for i in range(hp["rnn_layers"]):
                rnn_layer = tf.keras.layers.Bidirectional(
                    tf.keras.layers.GRU(
                        hp["rnn_units"],
                        return_sequences=True,
                        dropout=hp["dropout_rate"],
                        recurrent_dropout=hp["dropout_rate"]/2
                    )
                )(rnn_layer)
            
            # Attention mechanism
            attention = tf.keras.layers.Dense(1, activation='tanh')(rnn_layer)
            attention = tf.keras.layers.Flatten()(attention)
            attention_weights = tf.keras.layers.Activation('softmax')(attention)
            attention_weights = tf.keras.layers.RepeatVector(hp["rnn_units"]*2)(attention_weights)
            attention_weights = tf.keras.layers.Permute([2, 1])(attention_weights)
            
            attention_output = tf.keras.layers.Multiply()([rnn_layer, attention_weights])
            attention_pooled = tf.keras.layers.Lambda(lambda x: tf.keras.backend.sum(x, axis=1))(attention_output)
        else:
            # Simple LSTM without attention
            rnn_layer = inputs
            for i in range(hp["rnn_layers"]-1):
                rnn_layer = tf.keras.layers.LSTM(
                    hp["rnn_units"],
                    return_sequences=True,
                    dropout=hp["dropout_rate"],
                    recurrent_dropout=hp["dropout_rate"]/2
                )(rnn_layer)
            
            attention_pooled = tf.keras.layers.LSTM(
                hp["rnn_units"],
                dropout=hp["dropout_rate"],
                recurrent_dropout=hp["dropout_rate"]/2
            )(rnn_layer)
        
        # Merge branches if both are used
        if hp["use_tcn"] and (hp["use_attention"] or "attention_pooled" in locals()):
            merged = tf.keras.layers.Concatenate()([tcn_pooled, attention_pooled])
        elif hp["use_tcn"]:
            merged = tcn_pooled
        else:
            merged = attention_pooled
        
        # Dense layers
        dense = tf.keras.layers.Dense(128, activation='relu')(merged)
        dense = tf.keras.layers.Dropout(hp["dropout_rate"])(dense)
        dense = tf.keras.layers.Dense(64, activation='relu')(dense)
        dense = tf.keras.layers.Dropout(hp["dropout_rate"])(dense)
        
        # Output layer
        if output_shape == 1:
            outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense)
        else:
            outputs = tf.keras.layers.Dense(output_shape)(dense)
        
        # Create model
        model = tf.keras.Model(inputs=inputs, outputs=outputs)
        
        # Compile with Adam optimizer
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=hp["learning_rate"]),
            loss='mse',  # Mean squared error for regression
            metrics=['mae']  # Mean absolute error
        )
        
        return model
    
    def train_model(self, pair: str, timeframe: str = "1h") -> Dict[str, Any]:
        """
        Train ML model for a specific trading pair
        
        Args:
            pair: Trading pair
            timeframe: Timeframe
            
        Returns:
            Dictionary with training results
        """
        try:
            # Prepare data
            data = self.prepare_training_data(pair, timeframe)
            if data is None or len(data) < self.min_required_samples:
                logger.warning(f"Insufficient data for {pair}, skipping training")
                return None
            
            # Prepare model inputs
            X_train, y_train, X_val, y_val, X_test, y_test, feature_scaler = self.prepare_model_inputs(
                data, self.lookback, self.forecast_horizon
            )
            
            # Get optimal hyperparameters from tuner
            hyperparameters = self.hyperparameter_tuner.get_optimal_parameters()
            
            # Create model
            model = self.create_model(
                input_shape=(self.lookback, X_train.shape[2]),
                output_shape=y_train.shape[1],
                hyperparameters=hyperparameters
            )
            
            # Define callbacks
            callbacks = [
                tf.keras.callbacks.EarlyStopping(
                    monitor='val_loss',
                    patience=20,
                    restore_best_weights=True
                ),
                tf.keras.callbacks.ReduceLROnPlateau(
                    monitor='val_loss',
                    factor=0.5,
                    patience=10,
                    min_lr=1e-5
                )
            ]
            
            # Train model
            start_time = time.time()
            history = model.fit(
                X_train, y_train,
                validation_data=(X_val, y_val),
                epochs=self.epochs,
                batch_size=self.batch_size,
                callbacks=callbacks,
                verbose=2
            )
            training_time = time.time() - start_time
            
            # Evaluate model
            test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)
            
            # Save model
            model_path = self._save_model(model, pair, timeframe, feature_scaler)
            
            # Get predictions on test set
            test_predictions = model.predict(X_test)
            
            # Calculate binary accuracy for direction prediction
            direction_accuracy = self._calculate_direction_accuracy(y_test, test_predictions)
            
            # Calculate profit factor and other metrics
            profit_metrics = self._calculate_profit_metrics(y_test, test_predictions)
            
            # Save results
            results = {
                "pair": pair,
                "timeframe": timeframe,
                "training_samples": len(X_train),
                "validation_samples": len(X_val),
                "test_samples": len(X_test),
                "training_time": training_time,
                "test_loss": test_loss,
                "test_mae": test_mae,
                "direction_accuracy": direction_accuracy,
                "profit_metrics": profit_metrics,
                "model_path": model_path,
                "trained_at": datetime.now().isoformat()
            }
            
            # Save results to file
            self._save_results(results, pair, timeframe)
            
            logger.info(f"Model training completed for {pair}")
            logger.info(f"Direction accuracy: {direction_accuracy:.2f}%, Profit factor: {profit_metrics.get('profit_factor', 0):.2f}")
            
            return results
        
        except Exception as e:
            logger.error(f"Error training model for {pair}: {e}")
            return None
    
    def _save_model(self, model: tf.keras.Model, pair: str, timeframe: str, feature_scaler) -> str:
        """
        Save trained model and scaler
        
        Args:
            model: Trained Keras model
            pair: Trading pair
            timeframe: Timeframe
            feature_scaler: Feature scaler
            
        Returns:
            Path to saved model
        """
        # Create directory for this pair
        pair_dir = os.path.join(MODELS_DIR, pair.replace("/", ""))
        os.makedirs(pair_dir, exist_ok=True)
        
        # Create timestamped model path
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_path = os.path.join(pair_dir, f"enhanced_model_{timeframe}_{timestamp}")
        
        # Save model
        model.save(model_path)
        
        # Save scaler
        import joblib
        scaler_path = os.path.join(model_path, "feature_scaler.pkl")
        joblib.dump(feature_scaler, scaler_path)
        
        logger.info(f"Saved model to {model_path}")
        
        return model_path
    
    def _calculate_direction_accuracy(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:
        """
        Calculate accuracy for price direction prediction
        
        Args:
            y_true: True values
            y_pred: Predicted values
            
        Returns:
            Direction accuracy percentage
        """
        # Get direction from first target column (target_direction_X)
        true_direction = y_true[:, 0]
        pred_direction = (y_pred[:, 0] > 0.5).astype(int)
        
        # Calculate accuracy
        correct = np.sum(true_direction == pred_direction)
        accuracy = correct / len(true_direction) * 100
        
        return accuracy
    
    def _calculate_profit_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """
        Calculate profit metrics from predictions
        
        Args:
            y_true: True values
            y_pred: Predicted values
            
        Returns:
            Dictionary with profit metrics
        """
        # Get percentage change target (index 2 is target_pct_change_X)
        true_pct_change = y_true[:, 2]
        
        # Get direction prediction (index 0 is target_direction_X)
        pred_direction = (y_pred[:, 0] > 0.5).astype(int)
        
        # Calculate profit factor (considering only trades in predicted direction)
        long_returns = true_pct_change * (pred_direction == 1)
        short_returns = -true_pct_change * (pred_direction == 0)
        
        # Combined returns (long and short)
        returns = long_returns + short_returns
        
        # Calculate metrics
        win_rate = np.sum(returns > 0) / np.sum(returns != 0) * 100
        profit_factor = np.sum(returns[returns > 0]) / abs(np.sum(returns[returns < 0])) if np.sum(returns < 0) != 0 else float('inf')
        avg_win = np.mean(returns[returns > 0]) if any(returns > 0) else 0
        avg_loss = np.mean(returns[returns < 0]) if any(returns < 0) else 0
        
        # Calculate expectancy
        expectancy = (win_rate/100 * avg_win) - ((1 - win_rate/100) * abs(avg_loss))
        
        # Calculate Sharpe Ratio (simplified)
        sharpe = np.mean(returns) / np.std(returns) if np.std(returns) != 0 else 0
        
        return {
            "win_rate": win_rate,
            "profit_factor": profit_factor,
            "avg_win": avg_win,
            "avg_loss": avg_loss,
            "expectancy": expectancy,
            "sharpe_ratio": sharpe,
            "total_return": np.sum(returns)
        }
    
    def _save_results(self, results: Dict[str, Any], pair: str, timeframe: str) -> None:
        """
        Save training results to file
        
        Args:
            results: Training results
            pair: Trading pair
            timeframe: Timeframe
        """
        # Create results directory for this pair
        pair_dir = os.path.join(RESULTS_DIR, pair.replace("/", ""))
        os.makedirs(pair_dir, exist_ok=True)
        
        # Create timestamped results file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_path = os.path.join(pair_dir, f"training_results_{timeframe}_{timestamp}.json")
        
        # Save results
        with open(results_path, "w") as f:
            json.dump(results, f, indent=2)
        
        logger.info(f"Saved training results to {results_path}")
    
    def backtest_strategy(self, pair: str, timeframe: str = "1h") -> Dict[str, Any]:
        """
        Backtest enhanced strategy with realistic market simulation
        
        Args:
            pair: Trading pair
            timeframe: Timeframe
            
        Returns:
            Dictionary with backtest results
        """
        try:
            # Load data
            pair_filename = pair.replace("/", "")
            data_path = os.path.join(TRAINING_DATA_DIR, f"{pair_filename}_{timeframe}.csv")
            
            if not os.path.exists(data_path):
                logger.warning(f"Data file {data_path} not found")
                return None
            
            # Run backtesting
            backtest_results = self.backtester.run_backtest(
                pair=pair,
                data_path=data_path,
                strategy_type="enhanced",
                use_ml=True,
                timeframe=timeframe
            )
            
            # Calculate performance metrics
            metrics = self._calculate_backtest_metrics(backtest_results)
            
            # Save backtest results
            self._save_backtest_results(backtest_results, metrics, pair, timeframe)
            
            return {
                "pair": pair,
                "timeframe": timeframe,
                "metrics": metrics,
                "results": backtest_results
            }
        
        except Exception as e:
            logger.error(f"Error backtesting strategy for {pair}: {e}")
            return None
    
    def _calculate_backtest_metrics(self, backtest_results: Dict[str, Any]) -> Dict[str, float]:
        """
        Calculate performance metrics from backtest results
        
        Args:
            backtest_results: Backtest results
            
        Returns:
            Dictionary with performance metrics
        """
        trades = backtest_results.get("trades", [])
        if not trades:
            return {
                "total_trades": 0,
                "win_rate": 0,
                "profit_factor": 0,
                "total_return_pct": 0,
                "max_drawdown_pct": 0,
                "sharpe_ratio": 0,
                "avg_leverage": 0,
                "avg_win_pct": 0,
                "avg_loss_pct": 0
            }
        
        # Calculate metrics
        total_trades = len(trades)
        winning_trades = [t for t in trades if t.get("profit_pct", 0) > 0]
        losing_trades = [t for t in trades if t.get("profit_pct", 0) <= 0]
        
        win_rate = len(winning_trades) / total_trades * 100 if total_trades > 0 else 0
        
        total_profit = sum(t.get("profit_pct", 0) for t in winning_trades)
        total_loss = abs(sum(t.get("profit_pct", 0) for t in losing_trades))
        profit_factor = total_profit / total_loss if total_loss > 0 else float('inf')
        
        # Calculate return metrics
        equity_curve = backtest_results.get("equity_curve", [])
        total_return_pct = (equity_curve[-1] / equity_curve[0] - 1) * 100 if equity_curve else 0
        
        # Calculate drawdown
        peak = 0
        max_drawdown = 0
        for equity in equity_curve:
            if equity > peak:
                peak = equity
            drawdown = (peak - equity) / peak * 100
            max_drawdown = max(max_drawdown, drawdown)
        
        # Calculate Sharpe ratio
        returns = [equity_curve[i] / equity_curve[i-1] - 1 for i in range(1, len(equity_curve))]
        sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252) if returns and np.std(returns) > 0 else 0
        
        # Calculate leverage metrics
        avg_leverage = np.mean([t.get("leverage", 0) for t in trades])
        
        # Calculate average win/loss
        avg_win_pct = np.mean([t.get("profit_pct", 0) for t in winning_trades]) if winning_trades else 0
        avg_loss_pct = np.mean([t.get("profit_pct", 0) for t in losing_trades]) if losing_trades else 0
        
        return {
            "total_trades": total_trades,
            "win_rate": win_rate,
            "profit_factor": profit_factor,
            "total_return_pct": total_return_pct,
            "max_drawdown_pct": max_drawdown,
            "sharpe_ratio": sharpe,
            "avg_leverage": avg_leverage,
            "avg_win_pct": avg_win_pct,
            "avg_loss_pct": avg_loss_pct
        }
    
    def _save_backtest_results(self, backtest_results: Dict[str, Any], metrics: Dict[str, float], pair: str, timeframe: str) -> None:
        """
        Save backtest results to file
        
        Args:
            backtest_results: Backtest results
            metrics: Performance metrics
            pair: Trading pair
            timeframe: Timeframe
        """
        # Create results directory for this pair
        pair_dir = os.path.join(RESULTS_DIR, pair.replace("/", ""))
        os.makedirs(pair_dir, exist_ok=True)
        
        # Create timestamped results file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_path = os.path.join(pair_dir, f"backtest_results_{timeframe}_{timestamp}.json")
        
        # Save results
        with open(results_path, "w") as f:
            json.dump(
                {
                    "pair": pair,
                    "timeframe": timeframe,
                    "metrics": metrics,
                    "results_summary": {
                        "total_trades": backtest_results.get("total_trades", 0),
                        "winning_trades": backtest_results.get("winning_trades", 0),
                        "losing_trades": backtest_results.get("losing_trades", 0),
                        "total_profit": backtest_results.get("total_profit", 0),
                        "initial_capital": backtest_results.get("initial_capital", 0),
                        "final_capital": backtest_results.get("final_capital", 0)
                    },
                    "timestamp": datetime.now().isoformat()
                },
                f, indent=2
            )
        
        logger.info(f"Saved backtest results to {results_path}")
        logger.info(f"Backtest metrics for {pair}: Win rate: {metrics['win_rate']:.2f}%, Return: {metrics['total_return_pct']:.2f}%")
    
    def train_all_pairs(self) -> Dict[str, Dict[str, Any]]:
        """
        Train models for all trading pairs
        
        Returns:
            Dictionary with training results for each pair
        """
        results = {}
        
        for pair in self.trading_pairs:
            logger.info(f"Training model for {pair}...")
            pair_result = self.train_model(pair)
            
            if pair_result:
                # Backtest the strategy
                backtest_result = self.backtest_strategy(pair)
                
                # Combine results
                results[pair] = {
                    "training": pair_result,
                    "backtest": backtest_result
                }
                
                # Check if performance meets targets
                metrics = backtest_result.get("metrics", {}) if backtest_result else {}
                win_rate = metrics.get("win_rate", 0)
                total_return_pct = metrics.get("total_return_pct", 0)
                
                if win_rate >= self.target_win_rate * 100 and total_return_pct >= self.target_return_pct:
                    logger.info(f"Target performance achieved for {pair}! ðŸŽ‰")
                else:
                    logger.info(f"Performance not meeting targets for {pair}, win rate: {win_rate:.2f}%, return: {total_return_pct:.2f}%")
                    
                    # Adjust hyperparameters for improved performance
                    self._adjust_hyperparameters_for_pair(pair, win_rate, total_return_pct)
        
        # Summarize results
        self._summarize_results(results)
        
        return results
    
    def _adjust_hyperparameters_for_pair(self, pair: str, win_rate: float, return_pct: float) -> None:
        """
        Adjust hyperparameters to improve performance for a specific pair
        
        Args:
            pair: Trading pair
            win_rate: Current win rate
            return_pct: Current return percentage
        """
        # Load current config
        pair_key = pair.replace("/", "")
        if "asset_specific_settings" in self.config and pair in self.config["asset_specific_settings"]:
            pair_config = self.config["asset_specific_settings"][pair]
        else:
            pair_config = {}
            self.config.setdefault("asset_specific_settings", {})[pair] = pair_config
        
        # Adjust risk parameters based on performance
        risk_params = pair_config.setdefault("risk_params", {})
        
        if win_rate < self.target_win_rate * 70:  # Severely underperforming
            # Reduce leverage and increase confidence threshold
            risk_params["max_leverage"] = min(risk_params.get("max_leverage", 20), 10)
            risk_params["confidence_threshold"] = max(risk_params.get("confidence_threshold", 0.7), 0.8)
            logger.info(f"Severely reduced risk for {pair} due to low win rate")
            
        elif win_rate < self.target_win_rate * 90:  # Moderately underperforming
            # Slightly reduce leverage
            risk_params["max_leverage"] = min(risk_params.get("max_leverage", 20), 15)
            logger.info(f"Moderately reduced risk for {pair} due to low win rate")
            
        elif return_pct < self.target_return_pct * 0.5:  # Good accuracy but low returns
            # Increase leverage if win rate is good
            if win_rate >= self.target_win_rate * 95:
                current_leverage = risk_params.get("max_leverage", 20)
                risk_params["max_leverage"] = min(current_leverage * 1.5, self.max_leverage)
                logger.info(f"Increased leverage for {pair} to improve returns with good accuracy")
        
        # Save updated config
        with open(self.config_path, "w") as f:
            json.dump(self.config, f, indent=2)
            
        logger.info(f"Updated configuration for {pair}")
    
    def _summarize_results(self, results: Dict[str, Dict[str, Any]]) -> None:
        """
        Summarize training and backtest results
        
        Args:
            results: Results dictionary
        """
        summary = {
            "timestamp": datetime.now().isoformat(),
            "pairs_trained": len(results),
            "average_win_rate": 0,
            "average_return_pct": 0,
            "pairs_meeting_targets": 0,
            "best_performing_pair": None,
            "worst_performing_pair": None,
            "pair_results": {}
        }
        
        if not results:
            logger.warning("No results to summarize")
            return
        
        # Calculate averages and find best/worst pairs
        win_rates = []
        returns = []
        best_return = -float('inf')
        worst_return = float('inf')
        best_pair = None
        worst_pair = None
        
        for pair, pair_results in results.items():
            backtest = pair_results.get("backtest", {})
            metrics = backtest.get("metrics", {}) if backtest else {}
            win_rate = metrics.get("win_rate", 0)
            total_return_pct = metrics.get("total_return_pct", 0)
            
            win_rates.append(win_rate)
            returns.append(total_return_pct)
            
            # Check if meets targets
            if win_rate >= self.target_win_rate * 100 and total_return_pct >= self.target_return_pct:
                summary["pairs_meeting_targets"] += 1
            
            # Track best/worst
            if total_return_pct > best_return:
                best_return = total_return_pct
                best_pair = pair
            if total_return_pct < worst_return:
                worst_return = total_return_pct
                worst_pair = pair
            
            # Add to pair results
            summary["pair_results"][pair] = {
                "win_rate": win_rate,
                "return_pct": total_return_pct,
                "meets_targets": win_rate >= self.target_win_rate * 100 and total_return_pct >= self.target_return_pct
            }
        
        # Calculate averages
        summary["average_win_rate"] = np.mean(win_rates) if win_rates else 0
        summary["average_return_pct"] = np.mean(returns) if returns else 0
        summary["best_performing_pair"] = best_pair
        summary["worst_performing_pair"] = worst_pair
        
        # Save summary
        summary_path = os.path.join(RESULTS_DIR, f"training_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(summary_path, "w") as f:
            json.dump(summary, f, indent=2)
        
        # Log summary
        logger.info(f"Training summary: {len(results)} pairs trained")
        logger.info(f"Average win rate: {summary['average_win_rate']:.2f}%, Average return: {summary['average_return_pct']:.2f}%")
        logger.info(f"Pairs meeting targets: {summary['pairs_meeting_targets']} of {len(results)}")
        logger.info(f"Best performing pair: {best_pair} ({best_return:.2f}%)")
        logger.info(f"Worst performing pair: {worst_pair} ({worst_return:.2f}%)")

def main():
    """Main function to run the enhanced strategy training"""
    # Parse arguments
    import argparse
    parser = argparse.ArgumentParser(description="Enhanced Strategy Training")
    parser.add_argument("--pairs", type=str, nargs="+", default=["SOL/USD", "ETH/USD", "BTC/USD"],
                        help="Trading pairs to train on")
    parser.add_argument("--epochs", type=int, default=300, help="Number of training epochs")
    parser.add_argument("--batch-size", type=int, default=32, help="Training batch size")
    parser.add_argument("--lookback", type=int, default=100, help="Lookback period")
    parser.add_argument("--forecast", type=int, default=12, help="Forecast horizon")
    parser.add_argument("--max-leverage", type=int, default=125, help="Maximum leverage")
    parser.add_argument("--target-win-rate", type=float, default=0.9, help="Target win rate (0.0-1.0)")
    parser.add_argument("--target-return", type=float, default=1000.0, help="Target return percentage")
    args = parser.parse_args()
    
    # Create trainer
    trainer = EnhancedStrategyTrainer(
        trading_pairs=args.pairs,
        epochs=args.epochs,
        batch_size=args.batch_size,
        lookback=args.lookback,
        forecast_horizon=args.forecast,
        max_leverage=args.max_leverage,
        target_win_rate=args.target_win_rate,
        target_return_pct=args.target_return
    )
    
    # Train all pairs
    results = trainer.train_all_pairs()
    
    logger.info("Enhanced strategy training completed")

if __name__ == "__main__":
    main()